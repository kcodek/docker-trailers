filebeat.prospectors:
- type: log
  enabled: true
  tags:
    - netflower
  paths:
    - /usr/share/services/netflower/logs/netflower.log

- type: log
  enabled: true
  tags:
    - execer
  paths:
    - /usr/share/services/execer/logs/execer.log

output.kafka:
  # specifying filebeat to take timestamp and message fields, other wise it
  # take the lines as json and publish to kafka
  codec.format:
    string: '%{[@timestamp]} %{[message]}'

# initial brokers for reading cluster metadata
  # hosts: ["kafka1:9092", "kafka2:9092", "kafka3:9092"]
  hosts: ["kafka:9092"]

  # message topic selection + partitioning
  # topic: '%{[fields.log_topic]}'
  topic: log
  partition.round_robin:
    reachable_only: false

  required_acks: 1
  compression: gzip
  max_message_bytes: 1000000